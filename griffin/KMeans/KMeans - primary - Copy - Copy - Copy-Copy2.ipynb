{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    \n",
    "    def __init__(self, n_clusters = 8, dist = 'Euclid'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.cluster_centers = np.zeros((n_clusters,2))\n",
    "        if dist == 'Euclid':\n",
    "            self._distance = self._distance_euclid\n",
    "        elif dist == 'Geodesic':\n",
    "            self._distance = self._distance_haversine\n",
    "        \n",
    "    \n",
    "    def _has_converged(self,old_centers, new_centers):\n",
    "        return np.array_equal(old_centers, new_centers)\n",
    "    \n",
    "    def _compute_clusters(self, X):\n",
    "        cluster_list = np.zeros((len(X)),dtype=np.int)\n",
    "        for i,x in enumerate(X):\n",
    "            cluster_list[i] = np.argmin(\n",
    "                np.array([self._distance(x,self.cluster_centers[k])\n",
    "                          for k in range(self.n_clusters)]))\n",
    "        return cluster_list\n",
    "        \n",
    "    def _recompute_centers(self, X):\n",
    "        #centers = self.cluster_centers.copy()\n",
    "        centers = np.zeros((self.n_clusters,2))\n",
    "        for k in range(self.n_clusters):\n",
    "            points = []\n",
    "            for index, item in enumerate(self.labels):\n",
    "#                 print(f'Index = {index}')\n",
    "#                 print(f'Item = {item}')\n",
    "                if(item == k):\n",
    "#                     print(f'Point = {X[index]}')\n",
    "                    points.append(X[index])\n",
    "            points = np.array(points)\n",
    "      #      print(f'Mean: {np.mean(points, axis=0)}')\n",
    "            if(points.size == 0):\n",
    "                print(self.cluster_centers)\n",
    "                print(self.n_clusters)\n",
    "                print(self.labels)\n",
    "                print(np.mean(points, axis=0))\n",
    "            centers[k] = np.mean(points, axis=0)\n",
    "          #  centers.append(np.mean(points,axis=0))\n",
    "       # print(centers)\n",
    "        return centers\n",
    "        \n",
    "    def _distance_euclid(self, x,y):\n",
    "        return np.linalg.norm(np.subtract(x,y))\n",
    "    \n",
    "    \"\"\"\n",
    "    Expects points to be of the form lat,lon\n",
    "    \"\"\"\n",
    "    def _distance_haversine(self,x,y):\n",
    "        lat_1, lon_1, lat_2, lon_2 = map(np.radians,[x[0],x[1],y[0],y[1]])\n",
    "        d_lat = lat_2 - lat_1\n",
    "        d_lon = lon_2 - lon_1\n",
    "        \n",
    "        a = np.sin(d_lat/2.0)**2 + np.cos(lat_1)*np.cos(lat_2)*np.sin(d_lon/2)**2\n",
    "        \n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        km = 6372.8 * c\n",
    "        return km\n",
    "   \n",
    "    def _initialize_centers(self,X):\n",
    "        centers = []\n",
    "        length = len(X)\n",
    "        for k in range(self.n_clusters):\n",
    "            index = np.random.randint(0,length)\n",
    "            while (tuple(X[index]) in set(map(tuple,centers))):\n",
    "                index = np.random.randint(0,length)\n",
    "            centers.append(X[index])\n",
    "        return np.array(centers)\n",
    "\n",
    "\n",
    "    def fit(self,X):\n",
    "        self.X = X\n",
    "        old_centers = self.cluster_centers.copy()\n",
    "        self.cluster_centers = self._initialize_centers(X)\n",
    "       # print(f'Initialized Clusters: {self.cluster_centers}')\n",
    "        while(not self._has_converged(old_centers, self.cluster_centers.copy())):\n",
    "            old_centers = self.cluster_centers.copy()\n",
    "            self.labels = self._compute_clusters(X)\n",
    "            self.cluster_centers = self._recompute_centers(X)\n",
    "            \n",
    "    def fit_from_starting_points(self,X,starting_clusters):\n",
    "        self.X = X\n",
    "        old_centers = self.cluster_centers.copy()\n",
    "        self.cluster_centers = starting_clusters\n",
    "        while(not self._has_converged(old_centers, self.cluster_centers.copy())):\n",
    "            old_centers = self.cluster_centers.copy()\n",
    "            self.labels = self._compute_clusters(X)\n",
    "            self.cluster_centers = self._recompute_centers(X)\n",
    "            \n",
    "    def avg_distance(self):\n",
    "        centers = self.cluster_centers\n",
    "        k = self.n_clusters\n",
    "        avg_distance = 0\n",
    "        for cluster in range(k):\n",
    "            total = 0\n",
    "            dist = 0\n",
    "            for index, point in enumerate(self.labels):\n",
    "                if (point == cluster):\n",
    "                    total += 1\n",
    "                    dist += self._distance(self.X[index], centers[cluster])\n",
    "            avg_distance += dist / total\n",
    "        avg_distance = avg_distance / k\n",
    "        return avg_distance\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "demographics = gpd.read_file('./census.geoJSON')\n",
    "\n",
    "def gen_coords(loc):\n",
    "    data = loc[1:-1].split(',')\n",
    "    data = list((np.float(data[0]), np.float(data[1])))\n",
    "    x.append(data[1])\n",
    "    y.append(data[0])\n",
    "    return [data[0],data[1]]\n",
    "\n",
    "def point_similarity(X,geo_labels, euc_labels,k):\n",
    "    '''For an inputted series of points, geodesic labels, euclidean labels, and k-value\n",
    "       returns the point-similarity index per geodesic cluster\n",
    "    '''\n",
    "\n",
    "    euc_cluster_totals = np.zeros(k,dtype=np.int)\n",
    "    geo_euc_composition = [np.zeros(k,dtype=np.int)* 1 for i in range(k)]\n",
    "    \n",
    "    for index,point in enumerate(geo_labels):\n",
    "        euc_cluster_totals[euc_labels[index]] += 1\n",
    "        geo_euc_composition[point][euc_labels[index]] += 1\n",
    "    \n",
    "    point_sim = []\n",
    "    for geo_cluster in range(k):\n",
    "        sim = 0\n",
    "        for euc_cluster in range(k):\n",
    "            matching_points = geo_euc_composition[geo_cluster][euc_cluster]\n",
    "            euc_percentage = matching_points / euc_cluster_totals[euc_cluster]\n",
    "            geo_percentage = matching_points / np.sum(geo_euc_composition[geo_cluster])\n",
    "            sim += euc_percentage * geo_percentage\n",
    "        point_sim.append(sim)\n",
    "\n",
    "    return np.array(point_sim)\n",
    "\n",
    "def minority_probability(X,cluster_number,geo_labels,demographics):\n",
    "        points = X[geo_labels == cluster_number]\n",
    "        # geoJSON puts points in Long/Lat order\n",
    "        # but points are in lat/long earlier\n",
    "        hull = shapely.geometry.multipoint.MultiPoint([[p[1],p[0]] for p in points]).convex_hull\n",
    "  \n",
    "        pop = np.zeros(7)\n",
    "        for index in range(len(demographics)):\n",
    "            census_tract = demographics.loc[index,'geometry']\n",
    "            intersect = hull.intersection(census_tract)\n",
    "            overlap = intersect.area/census_tract.area\n",
    "            if (overlap != 0):\n",
    "                pop = pop + (np.array(demographics.loc[index,['White','Black or African American', 'American Indian and Ala Native',\n",
    "                   'Asian','Native Hawaiian/other Pac Isl', 'Multiple Race',\n",
    "                   'Other Race']]) * overlap)\n",
    "                            \n",
    "        return (pop[1:]/np.sum(pop)).sum()\n",
    "\n",
    "def bias_index(X, geo_labels, euc_labels, demographics, k):\n",
    "\n",
    "    dissimilarity_index = 1 - point_similarity(X,geo_labels,euc_labels,k)\n",
    "    minority_prob = np.array([minority_probability(X,cluster,geo_labels,demographics) \n",
    "                              for cluster in range(k)])\n",
    "    potential_bias = minority_prob * dissimilarity_index\n",
    "    return potential_bias.mean()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment_heap/data_2010_m_theft_april_2.csv\n",
      "./experiment_heap/data_2010_m_theft_april_3.csv\n",
      "./experiment_heap/data_2010_m_theft_april_4.csv\n",
      "./experiment_heap/data_2010_m_theft_april_5.csv\n",
      "./experiment_heap/data_2010_m_theft_april_6.csv\n",
      "./experiment_heap/data_2010_m_theft_april_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment_heap/data_2010_m_theft_april_8.csv\n",
      "./experiment_heap/data_2010_m_theft_april_9.csv\n",
      "./experiment_heap/data_2010_m_theft_april_10.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_2.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_3.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_4.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_5.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_6.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_7.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_8.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_9.csv\n",
      "./experiment_heap/data_2010_m_theft_aug_10.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_2.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_3.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_4.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_5.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_6.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_7.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_8.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_9.csv\n",
      "./experiment_heap/data_2010_m_theft_dec_10.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_2.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_3.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_4.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_5.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_6.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_7.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_8.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_9.csv\n",
      "./experiment_heap/data_2010_m_theft_feb_10.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_2.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_3.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_4.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_5.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_6.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_7.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_8.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_9.csv\n",
      "./experiment_heap/data_2010_m_theft_jan_10.csv\n",
      "./experiment_heap/data_2010_m_theft_july_2.csv\n",
      "./experiment_heap/data_2010_m_theft_july_3.csv\n",
      "./experiment_heap/data_2010_m_theft_july_4.csv\n",
      "./experiment_heap/data_2010_m_theft_july_5.csv\n",
      "./experiment_heap/data_2010_m_theft_july_6.csv\n",
      "./experiment_heap/data_2010_m_theft_july_7.csv\n",
      "./experiment_heap/data_2010_m_theft_july_8.csv\n",
      "./experiment_heap/data_2010_m_theft_july_9.csv\n",
      "./experiment_heap/data_2010_m_theft_july_10.csv\n",
      "./experiment_heap/data_2010_m_theft_june_2.csv\n",
      "./experiment_heap/data_2010_m_theft_june_3.csv\n",
      "./experiment_heap/data_2010_m_theft_june_4.csv\n",
      "./experiment_heap/data_2010_m_theft_june_5.csv\n",
      "./experiment_heap/data_2010_m_theft_june_6.csv\n",
      "./experiment_heap/data_2010_m_theft_june_7.csv\n",
      "./experiment_heap/data_2010_m_theft_june_8.csv\n",
      "./experiment_heap/data_2010_m_theft_june_9.csv\n",
      "./experiment_heap/data_2010_m_theft_june_10.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_2.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_3.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_4.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_5.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_6.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_7.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_8.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_9.csv\n",
      "./experiment_heap/data_2010_m_theft_mar_10.csv\n",
      "./experiment_heap/data_2010_m_theft_may_2.csv\n",
      "./experiment_heap/data_2010_m_theft_may_3.csv\n",
      "./experiment_heap/data_2010_m_theft_may_4.csv\n",
      "./experiment_heap/data_2010_m_theft_may_5.csv\n",
      "./experiment_heap/data_2010_m_theft_may_6.csv\n",
      "./experiment_heap/data_2010_m_theft_may_7.csv\n",
      "./experiment_heap/data_2010_m_theft_may_8.csv\n",
      "./experiment_heap/data_2010_m_theft_may_9.csv\n",
      "./experiment_heap/data_2010_m_theft_may_10.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_2.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_3.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_4.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_5.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_6.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_7.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_8.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_9.csv\n",
      "./experiment_heap/data_2010_m_theft_nov_10.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_2.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_3.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_4.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_5.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_6.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_7.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_8.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_9.csv\n",
      "./experiment_heap/data_2010_m_theft_oct_10.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_2.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_3.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_4.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_5.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_6.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_7.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_8.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_9.csv\n",
      "./experiment_heap/data_2010_m_theft_sep_10.csv\n",
      "./experiment_heap/data_2010_theft_april_2.csv\n",
      "./experiment_heap/data_2010_theft_april_3.csv\n",
      "./experiment_heap/data_2010_theft_april_4.csv\n",
      "./experiment_heap/data_2010_theft_april_5.csv\n",
      "./experiment_heap/data_2010_theft_april_6.csv\n",
      "./experiment_heap/data_2010_theft_april_7.csv\n",
      "./experiment_heap/data_2010_theft_april_8.csv\n",
      "./experiment_heap/data_2010_theft_april_9.csv\n",
      "./experiment_heap/data_2010_theft_april_10.csv\n",
      "./experiment_heap/data_2010_theft_aug_2.csv\n",
      "./experiment_heap/data_2010_theft_aug_3.csv\n",
      "./experiment_heap/data_2010_theft_aug_4.csv\n",
      "./experiment_heap/data_2010_theft_aug_5.csv\n",
      "./experiment_heap/data_2010_theft_aug_6.csv\n",
      "./experiment_heap/data_2010_theft_aug_7.csv\n",
      "./experiment_heap/data_2010_theft_aug_8.csv\n",
      "./experiment_heap/data_2010_theft_aug_9.csv\n",
      "./experiment_heap/data_2010_theft_aug_10.csv\n",
      "./experiment_heap/data_2010_theft_dec_2.csv\n",
      "./experiment_heap/data_2010_theft_dec_3.csv\n",
      "./experiment_heap/data_2010_theft_dec_4.csv\n",
      "./experiment_heap/data_2010_theft_dec_5.csv\n",
      "./experiment_heap/data_2010_theft_dec_6.csv\n",
      "./experiment_heap/data_2010_theft_dec_7.csv\n",
      "./experiment_heap/data_2010_theft_dec_8.csv\n",
      "./experiment_heap/data_2010_theft_dec_9.csv\n",
      "./experiment_heap/data_2010_theft_dec_10.csv\n",
      "./experiment_heap/data_2010_theft_feb_2.csv\n",
      "./experiment_heap/data_2010_theft_feb_3.csv\n",
      "./experiment_heap/data_2010_theft_feb_4.csv\n",
      "./experiment_heap/data_2010_theft_feb_5.csv\n",
      "./experiment_heap/data_2010_theft_feb_6.csv\n",
      "./experiment_heap/data_2010_theft_feb_7.csv\n",
      "./experiment_heap/data_2010_theft_feb_8.csv\n",
      "./experiment_heap/data_2010_theft_feb_9.csv\n",
      "./experiment_heap/data_2010_theft_feb_10.csv\n",
      "./experiment_heap/data_2010_theft_jan_2.csv\n",
      "./experiment_heap/data_2010_theft_jan_3.csv\n",
      "./experiment_heap/data_2010_theft_jan_4.csv\n",
      "./experiment_heap/data_2010_theft_jan_5.csv\n",
      "./experiment_heap/data_2010_theft_jan_6.csv\n",
      "./experiment_heap/data_2010_theft_jan_7.csv\n",
      "./experiment_heap/data_2010_theft_jan_8.csv\n",
      "./experiment_heap/data_2010_theft_jan_9.csv\n",
      "./experiment_heap/data_2010_theft_jan_10.csv\n",
      "./experiment_heap/data_2010_theft_july_2.csv\n",
      "./experiment_heap/data_2010_theft_july_3.csv\n",
      "./experiment_heap/data_2010_theft_july_4.csv\n",
      "./experiment_heap/data_2010_theft_july_5.csv\n",
      "./experiment_heap/data_2010_theft_july_6.csv\n",
      "./experiment_heap/data_2010_theft_july_7.csv\n",
      "./experiment_heap/data_2010_theft_july_8.csv\n",
      "./experiment_heap/data_2010_theft_july_9.csv\n",
      "./experiment_heap/data_2010_theft_july_10.csv\n",
      "./experiment_heap/data_2010_theft_june_2.csv\n",
      "./experiment_heap/data_2010_theft_june_3.csv\n",
      "./experiment_heap/data_2010_theft_june_4.csv\n",
      "./experiment_heap/data_2010_theft_june_5.csv\n",
      "./experiment_heap/data_2010_theft_june_6.csv\n",
      "./experiment_heap/data_2010_theft_june_7.csv\n",
      "./experiment_heap/data_2010_theft_june_8.csv\n",
      "./experiment_heap/data_2010_theft_june_9.csv\n",
      "./experiment_heap/data_2010_theft_june_10.csv\n",
      "./experiment_heap/data_2010_theft_mar_2.csv\n",
      "./experiment_heap/data_2010_theft_mar_3.csv\n",
      "./experiment_heap/data_2010_theft_mar_4.csv\n",
      "./experiment_heap/data_2010_theft_mar_5.csv\n",
      "./experiment_heap/data_2010_theft_mar_6.csv\n",
      "./experiment_heap/data_2010_theft_mar_7.csv\n",
      "./experiment_heap/data_2010_theft_mar_8.csv\n",
      "./experiment_heap/data_2010_theft_mar_9.csv\n",
      "./experiment_heap/data_2010_theft_mar_10.csv\n",
      "./experiment_heap/data_2010_theft_may_2.csv\n",
      "./experiment_heap/data_2010_theft_may_3.csv\n",
      "./experiment_heap/data_2010_theft_may_4.csv\n",
      "./experiment_heap/data_2010_theft_may_5.csv\n",
      "./experiment_heap/data_2010_theft_may_6.csv\n",
      "./experiment_heap/data_2010_theft_may_7.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./experiment_heap/data_2010_theft_may_8.csv\n",
      "./experiment_heap/data_2010_theft_may_9.csv\n",
      "./experiment_heap/data_2010_theft_may_10.csv\n",
      "./experiment_heap/data_2010_theft_nov_2.csv\n",
      "./experiment_heap/data_2010_theft_nov_3.csv\n",
      "./experiment_heap/data_2010_theft_nov_4.csv\n",
      "./experiment_heap/data_2010_theft_nov_5.csv\n",
      "./experiment_heap/data_2010_theft_nov_6.csv\n",
      "./experiment_heap/data_2010_theft_nov_7.csv\n",
      "./experiment_heap/data_2010_theft_nov_8.csv\n",
      "./experiment_heap/data_2010_theft_nov_9.csv\n",
      "./experiment_heap/data_2010_theft_nov_10.csv\n",
      "./experiment_heap/data_2010_theft_oct_2.csv\n",
      "./experiment_heap/data_2010_theft_oct_3.csv\n",
      "./experiment_heap/data_2010_theft_oct_4.csv\n",
      "./experiment_heap/data_2010_theft_oct_5.csv\n",
      "./experiment_heap/data_2010_theft_oct_6.csv\n",
      "./experiment_heap/data_2010_theft_oct_7.csv\n",
      "./experiment_heap/data_2010_theft_oct_8.csv\n",
      "./experiment_heap/data_2010_theft_oct_9.csv\n",
      "./experiment_heap/data_2010_theft_oct_10.csv\n",
      "./experiment_heap/data_2010_theft_sep_2.csv\n",
      "./experiment_heap/data_2010_theft_sep_3.csv\n",
      "./experiment_heap/data_2010_theft_sep_4.csv\n",
      "./experiment_heap/data_2010_theft_sep_5.csv\n",
      "./experiment_heap/data_2010_theft_sep_6.csv\n",
      "./experiment_heap/data_2010_theft_sep_7.csv\n",
      "./experiment_heap/data_2010_theft_sep_8.csv\n",
      "./experiment_heap/data_2010_theft_sep_9.csv\n",
      "./experiment_heap/data_2010_theft_sep_10.csv\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir('../data/'):\n",
    "    if(folder != 'data_2010'):\n",
    "        continue\n",
    "    for file in os.listdir('../data/' + folder):\n",
    "        if(file.endswith('.csv')):\n",
    "            df = pd.read_csv('../data/' + folder +'/' + file, sep =';')\n",
    "        \n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            df['Points'] = df['Location'].apply(gen_coords)\n",
    "            points = [Point(xy) for xy in zip(x,y)]\n",
    "            crs = {'init': 'epsg:4326'}\n",
    "            geo_df = GeoDataFrame(df,crs=crs, geometry=points)\n",
    "            theft_both = geo_df.copy()\n",
    "            \n",
    "            X = np.array(theft_both['Points'])\n",
    "            for k in range(2,11):\n",
    "                bias_data = f'{folder}-{file};{k};'\n",
    "                for iteration in range(49):\n",
    "                    kmeans_theft_euclid = KMeans(n_clusters = k)\n",
    "                    kmeans_theft_geodesic = KMeans(n_clusters = k, dist = 'Geodesic')\n",
    "                    centers = kmeans_theft_geodesic._initialize_centers(X)\n",
    "\n",
    "                    kmeans_theft_euclid.fit_from_starting_points(X,centers)\n",
    "                    kmeans_theft_geodesic.fit_from_starting_points(X,centers) \n",
    "                    \n",
    "                    bias = bias_index(X,kmeans_theft_geodesic.labels,kmeans_theft_euclid.labels,demographics,k)\n",
    "                    bias_data += str(bias) +';'\n",
    "                    file_name = file.split('.csv')[0]\n",
    "                with open(f'./experiment_heap/{folder}_{file_name}_{k}.csv','w') as f:\n",
    "                    print(f'./experiment_heap/{folder}_{file_name}_{k}.csv')\n",
    "                    f.write(bias_data)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def gen_coords(loc):\n",
    "    data = loc[1:-1].split(',')\n",
    "    data = list((np.float(data[0]), np.float(data[1])))\n",
    "    x.append(data[1])\n",
    "    y.append(data[0])\n",
    "    return [data[0],data[1]]\n",
    "\n",
    "def percent_similarity(set_1, set_2):\n",
    "    count = 0\n",
    "    for i in range (len(set_1)):\n",
    "        if set_1[i] == set_2[i]:\n",
    "            count += 1\n",
    "    return count / len(set_1)\n",
    "\n",
    "\n",
    "for file in os.listdir('../data/data_2015/'):\n",
    "    if(file.endswith('.csv')):\n",
    "        df = pd.read_csv('../data/data_2015/' + file, sep =';')\n",
    "        \n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        df['Points'] = df['Location'].apply(gen_coords)\n",
    "        points = [Point(xy) for xy in zip(x,y)]\n",
    "        crs = {'init': 'epsg:4326'}\n",
    "        geo_df = GeoDataFrame(df,crs=crs, geometry=points)\n",
    "        theft_both = geo_df.copy()\n",
    "\n",
    "        X = np.array(theft_both['Points'])\n",
    "\n",
    "        for k in range(2,11):\n",
    "            while(True):\n",
    "                kmeans_theft_euclid = KMeans(n_clusters = k)\n",
    "                kmeans_theft_geodesic = KMeans(n_clusters = k, dist = 'Geodesic')\n",
    "                centers = kmeans_theft_geodesic._initialize_centers(X)\n",
    "\n",
    "                kmeans_theft_euclid.fit_from_starting_points(X,centers)\n",
    "                kmeans_theft_geodesic.fit_from_starting_points(X,centers) \n",
    "                if(percent_similarity(kmeans_theft_euclid.labels,\n",
    "                                     kmeans_theft_geodesic.labels) > .50):\n",
    "                    break\n",
    "\n",
    "            theft_both.loc[:,'e_cluster' + 'K' + str(k)] = kmeans_theft_euclid.labels.copy()\n",
    "            theft_both.loc[:,'g_cluster' + 'K' + str(k)] = kmeans_theft_geodesic.labels.copy()\n",
    "\n",
    "                \n",
    "            print(percent_similarity(kmeans_theft_euclid.labels,\n",
    "                                     kmeans_theft_geodesic.labels))\n",
    "\n",
    "            \n",
    "            \n",
    "        theft_both = theft_both.drop('Points', axis=1)\n",
    "\n",
    "        try:\n",
    "            os.remove('./datamound/'+file.split('.csv')[0] + '.js')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "        theft_both.to_file('./datamound/'+file.split('.csv')[0] + '.js', driver='GeoJSON')\n",
    "#         with open('./datamound/'+file.split('.csv')[0] + '.js', 'r') as original: data = original.read()\n",
    "#         with open('./datamound/'+file.split('.csv')[0] + '.js', 'w') as modified: modified.write('var both =' \n",
    "#                                                         + data +';')\n",
    "        print(file)\n",
    "        print('-------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "ordered_names = ['theft_jan.js','theft_feb.js','theft_mar.js','theft_april.js',\n",
    "                'theft_may.js','theft_june.js','theft_july.js','theft_aug.js',\n",
    "                'theft_sep.js','theft_oct.js','theft_nov.js','theft_dec.js',\n",
    "                'm_theft_jan.js','m_theft_feb.js','m_theft_mar.js','m_theft_april.js',\n",
    "                'm_theft_may.js','m_theft_june.js','m_theft_july.js','m_theft_aug.js',\n",
    "                'm_theft_sep.js','m_theft_oct.js','m_theft_nov.js','m_theft_dec.js']\n",
    "data = 'var complete_data =['\n",
    "for file in ordered_names:\n",
    "    reader = open('./datamound/' + file,'r')\n",
    "    data += (reader.read() + ',')\n",
    "    reader.close()\n",
    "    print(file)\n",
    "        \n",
    "writer = open('all.js','w')\n",
    "writer.write(data + '];')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
